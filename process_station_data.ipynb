{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Station Data\n",
    "\n",
    "This notebook loads Benchmark, GCP, and Gravity data, adds `status_tag` and `epoch` columns based on the measurement/observation date, and saves the processed data to new CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define file paths relative to the notebook location or use absolute paths\n",
    "# Assuming the notebook is in 'PGRS Portal' and data is in 'webGNIS Drive'\n",
    "base_path = os.path.dirname(os.getcwd()) # Get parent directory if notebook is in PGRS Portal\n",
    "# Or uncomment and set absolute path if needed:\n",
    "# base_path = 'c:\\Users\\cumbe\\OneDrive\\Desktop\\PGRS Portal' \n",
    "\n",
    "input_dir = os.path.join(base_path, 'webGNIS Drive')\n",
    "output_dir = os.path.join(base_path, 'webGNIS Drive') # Save processed files in the same directory\n",
    "\n",
    "benchmark_file = os.path.join(input_dir, 'Benchmarks_Mar2025.csv')\n",
    "gcp_file = os.path.join(input_dir, 'GCPs_Mar2025.csv')\n",
    "gravity_file = os.path.join(input_dir, 'Gravity.csv')\n",
    "\n",
    "output_benchmark_file = os.path.join(output_dir, 'Benchmarks_Processed.csv')\n",
    "output_gcp_file = os.path.join(output_dir, 'GCPs_Processed.csv')\n",
    "output_gravity_file = os.path.join(output_dir, 'Gravity_Processed.csv')\n",
    "\n",
    "# Ensure output directory exists (optional, if different from input)\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f'Input Directory: {input_dir}')\n",
    "print(f'Output Directory: {output_dir}')\n",
    "print(f'Benchmark Input: {benchmark_file}')\n",
    "print(f'GCP Input: {gcp_file}')\n",
    "print(f'Gravity Input: {gravity_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine status_tag and epoch based on year\n",
    "def get_status_epoch(year):\n",
    "    if pd.isna(year):\n",
    "        return 'Unknown', 'Unknown'\n",
    "    try:\n",
    "        year = int(year)\n",
    "        if year >= 2020:\n",
    "            status_tag = 'Current'\n",
    "            epoch = '2020'\n",
    "        elif year >= 2000:\n",
    "            status_tag = 'Old'\n",
    "            epoch = '2000'\n",
    "        else:\n",
    "            status_tag = 'Historical'\n",
    "            epoch = '1900'\n",
    "        return status_tag, epoch\n",
    "    except ValueError:\n",
    "        return 'Invalid Year', 'Invalid Year'\n",
    "\n",
    "# Function to process a dataframe with a date column\n",
    "def process_data(df, date_col, date_format=None):\n",
    "    # Convert date column to datetime objects, coercing errors\n",
    "    if date_format:\n",
    "        df['datetime_obj'] = pd.to_datetime(df[date_col], format=date_format, errors='coerce')\n",
    "    else:\n",
    "        df['datetime_obj'] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "        \n",
    "    # Extract year\n",
    "    df['year'] = df['datetime_obj'].dt.year\n",
    "    \n",
    "    # Apply the function to get status_tag and epoch\n",
    "    df[['status_tag', 'epoch']] = df['year'].apply(lambda y: pd.Series(get_status_epoch(y)))\n",
    "    \n",
    "    # Drop temporary columns\n",
    "    df = df.drop(columns=['datetime_obj', 'year'])\n",
    "    return df\n",
    "\n",
    "# Function to process gravity data (uses YEAR_MEASURED directly)\n",
    "def process_gravity_data(df, year_col):\n",
    "    # Apply the function directly to the year column\n",
    "    df[['status_tag', 'epoch']] = df[year_col].apply(lambda y: pd.Series(get_status_epoch(y)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Benchmark Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_bench = pd.read_csv(benchmark_file)\n",
    "    print(f'Loaded {benchmark_file} successfully.')\n",
    "    # Remove rows where both longitude and latitude are exactly zero\n",
    "    if 'longitude' in df_bench.columns and 'latitude' in df_bench.columns:\n",
    "        df_bench = df_bench[~((df_bench['longitude'] == 0) & (df_bench['latitude'] == 0))]\n",
    "    # Assuming date format is MM/DD/YYYY based on SQL script\n",
    "    df_bench_processed = process_data(df_bench.copy(), 'observation_date', date_format='%m/%d/%Y')\n",
    "    df_bench_processed.to_csv(output_benchmark_file, index=False)\n",
    "    print(f'Processed benchmark data saved to {output_benchmark_file}')\n",
    "    print(df_bench_processed[['observation_date', 'status_tag', 'epoch']].head())\n",
    "except FileNotFoundError:\n",
    "    print(f'Error: Benchmark file not found at {benchmark_file}')\n",
    "except KeyError:\n",
    "    print(f'Error: \'observation_date\' column not found in {benchmark_file}. Please check column names.')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred processing {benchmark_file}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process GCP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_gcp = pd.read_csv(gcp_file)\n",
    "    print(f'Loaded {gcp_file} successfully.')\n",
    "    # Remove rows where both longitude and latitude are exactly zero\n",
    "    if 'longitude' in df_gcp.columns and 'latitude' in df_gcp.columns:\n",
    "        df_gcp = df_gcp[~((df_gcp['longitude'] == 0) & (df_gcp['latitude'] == 0))]\n",
    "    # Assuming the date column is named 'observation_date' and format is MM/DD/YYYY \n",
    "    # *** Adjust 'observation_date' and date_format if necessary for GCPs_Mar2025.csv ***\n",
    "    date_column_gcp = 'observation_date' # <-- CHECK AND CHANGE IF NEEDED\n",
    "    date_format_gcp = '%m/%d/%Y'      # <-- CHECK AND CHANGE IF NEEDED\n",
    "    \n",
    "    if date_column_gcp not in df_gcp.columns:\n",
    "        print(f'Warning: Column \'{date_column_gcp}\' not found in {gcp_file}. Trying common alternatives...')\n",
    "        # Add potential alternative date column names here if known\n",
    "        possible_date_cols = ['DATE_OBSERVED', 'Date', 'ObservationDate', 'date']\n",
    "        found = False\n",
    "        for col in possible_date_cols:\n",
    "            if col in df_gcp.columns:\n",
    "                date_column_gcp = col\n",
    "                print(f'Using column \'{date_column_gcp}\' for GCP dates.')\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "             raise KeyError(f'Could not find a suitable date column in {gcp_file}. Please specify the correct column name.')\n",
    "\n",
    "    df_gcp_processed = process_data(df_gcp.copy(), date_column_gcp, date_format=date_format_gcp)\n",
    "    df_gcp_processed.to_csv(output_gcp_file, index=False)\n",
    "    print(f'Processed GCP data saved to {output_gcp_file}')\n",
    "    print(df_gcp_processed[[date_column_gcp, 'status_tag', 'epoch']].head())\n",
    "except FileNotFoundError:\n",
    "    print(f'Error: GCP file not found at {gcp_file}')\n",
    "except KeyError as e:\n",
    "     print(f'Error: {e} Please check column names in {gcp_file}.')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred processing {gcp_file}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Gravity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_grav = pd.read_csv(gravity_file)\n",
    "    print(f'Loaded {gravity_file} successfully.')\n",
    "    # Remove rows where both longitude and latitude are exactly zero\n",
    "    if 'LONGITUDE' in df_grav.columns and 'LATITUDE' in df_grav.columns:\n",
    "        df_grav = df_grav[~((df_grav['LONGITUDE'] == 0) & (df_grav['LATITUDE'] == 0))]\n",
    "    # Use the dedicated function for gravity data using 'YEAR_MEASURED'\n",
    "    df_grav_processed = process_gravity_data(df_grav.copy(), 'YEAR_MEASURED')\n",
    "    df_grav_processed.to_csv(output_gravity_file, index=False)\n",
    "    print(f'Processed gravity data saved to {output_gravity_file}')\n",
    "    print(df_grav_processed[['YEAR_MEASURED', 'status_tag', 'epoch']].head())\n",
    "except FileNotFoundError:\n",
    "    print(f'Error: Gravity file not found at {gravity_file}')\n",
    "except KeyError:\n",
    "    print(f'Error: \'YEAR_MEASURED\' column not found in {gravity_file}. Please check column names.')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred processing {gravity_file}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finished\n",
    "\n",
    "Processing complete. The new files with `status_tag` and `epoch` columns are:\n",
    "* `Benchmarks_Processed.csv`\n",
    "* `GCPs_Processed.csv`\n",
    "* `Gravity_Processed.csv`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x.x" 
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
